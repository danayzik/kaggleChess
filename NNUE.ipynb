{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyP1y0dv8PPWGfO7qYa6N2Hv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Important Note:\n"],"metadata":{"id":"j1B9X2oXUoTl"}},{"cell_type":"markdown","source":["Important Note:\n","If you wish to run the training yourself, it's recommended to swap the Runtime to gpu. Do this by going to Runtime -> change runtime type\n"],"metadata":{"id":"jUlOsvxuUs9n"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"TRyUI_3t-4RW"},"outputs":[],"source":["!pip install chess\n","!pip install gdown"]},{"cell_type":"markdown","source":["Imports"],"metadata":{"id":"M8npLF5xyaGP"}},{"cell_type":"code","source":["import chess\n","import gdown\n","import numpy as np\n","from google.colab import files\n","import torch\n","import pandas as pd\n","from torch.optim.lr_scheduler import StepLR\n","from torch.optim import RMSprop\n","import time\n","from tqdm import tqdm"],"metadata":{"id":"puqLrBes-8Ds"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download the data.csv file"],"metadata":{"id":"jMG2rbexxkZh"}},{"cell_type":"code","source":["data_file_id = \"1JEzNboVftrLKe1YptqIYqAu5T-IE76PH\"\n","url = f'https://drive.google.com/uc?id={data_file_id}'\n","gdown.download(url, 'data.csv', quiet=False)"],"metadata":{"id":"s3I7PWYLx7cv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download our trained model"],"metadata":{"id":"CRjpZuLWx7uh"}},{"cell_type":"code","source":["trained_model_file_id = \"1GfSRtp9IOZ3mfExnWaRLZ35Fm2Bwe7rn\"\n","url = f'https://drive.google.com/uc?id={trained_model_file_id}'\n","gdown.download(url, 'model.pth', quiet=False)"],"metadata":{"id":"8Mo8XzMXxk21"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Piece encoding with perspective shift for black, and input builder function"],"metadata":{"id":"Di95xWSWBj5K"}},{"cell_type":"code","source":["def calculate_index(sqr_index, piece_type, side, side_to_move) -> int:\n","    if side_to_move == chess.BLACK:\n","        side = 1 - side\n","        sqr_index ^= 0b111000\n","\n","    return side * 64 * 6 + (piece_type - 1) * 64 + sqr_index\n","\n","\n","def fen_to_feature_vector(fen: str) -> np.ndarray:\n","    board = chess.Board(fen)\n","    vec = np.zeros(768, dtype=np.float32)\n","    for piece_type in range(1, 7):\n","        for color in [chess.WHITE, chess.BLACK]:\n","            bb = int(board.pieces(piece_type, color))\n","            while bb:\n","                sq = (bb & -bb).bit_length() - 1\n","                idx = calculate_index(sq, piece_type, int(color), int(board.turn))\n","                vec[idx] = 1.0\n","                bb &= bb - 1\n","    return vec\n","\n"],"metadata":{"id":"AceqhDSL-8Ev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read the data"],"metadata":{"id":"_suAQk4GKhg_"}},{"cell_type":"code","source":["df = pd.read_csv('/content/data.csv')\n","print(df.shape)"],"metadata":{"id":"4DBH2SkGI1Kk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Flip the evaluation if its blacks turn. Our network is always from the perspective of side to move."],"metadata":{"id":"nwSSGdwi9ZI2"}},{"cell_type":"code","source":["df['Evaluation'] = df.apply(lambda row: -row['Evaluation'] if row['FEN'].split()[1] == 'b' else row['Evaluation'], axis=1)"],"metadata":{"id":"7daJ-7ai9YeJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split the data to train, validation, test"],"metadata":{"id":"KLYEj6n8MqTP"}},{"cell_type":"code","source":["split_train = 0.8\n","split_val = 0.1\n","split_test = 1 - split_train - split_val\n","\n","n = df.shape[0]\n","m_train, m_val = int(n * split_train), int(n * split_val)\n","train_df = df[:m_train].copy()\n","val_df = df[m_train:m_train + m_val].copy()\n","test_df = df[m_train + m_val:].copy()\n","\n","train_df.shape, val_df.shape, test_df.shape"],"metadata":{"id":"aIAgLwqULO4B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Normilize our targets for smoother training. Training mean and std values will be used for denormilizing our predicition"],"metadata":{"id":"XCsw69paVaNb"}},{"cell_type":"code","source":["y_train = train_df['Evaluation'].values.astype(np.float32)\n","y_val =  val_df['Evaluation'].values.astype(np.float32)\n","y_test =  test_df['Evaluation'].values.astype(np.float32)\n","y_train_mean = y_train.mean()\n","y_train_std = y_train.std()\n","y_val_mean = y_val.mean()\n","y_val_std = y_val.std()\n","y_test_mean = y_test.mean()\n","y_test_std = y_test.std()\n","\n","train_df['Evaluation'] = (train_df['Evaluation'] - y_train_mean) / y_train_std\n","val_df['Evaluation'] = (val_df['Evaluation'] - y_val_mean) / y_val_std\n","test_df['Evaluation'] = (test_df['Evaluation'] - y_test_mean) / y_test_std\n"],"metadata":{"id":"X-xxvKoOUqo3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define a dataset class that will use our board encoder"],"metadata":{"id":"SJX94ZSJSxZk"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","\n","class ChessFENDataset(Dataset):\n","    def __init__(self, fens, evals, encoder_fn):\n","        self.fens = fens\n","        self.evals = evals\n","        self.encoder = encoder_fn\n","\n","    def __len__(self):\n","        return len(self.fens)\n","\n","    def __getitem__(self, idx):\n","        x = torch.tensor(self.encoder(self.fens[idx]), dtype=torch.float32)\n","        y = torch.tensor(self.evals[idx], dtype=torch.float32)\n","        return x, y"],"metadata":{"id":"o7cLKVSsMtJs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define the model. When playing, the forward function will make use of efficient updates, thus speeding up evaluation"],"metadata":{"id":"4iqF6k2xTB4w"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class NNUE(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.fc1 = nn.Linear(768, 512)\n","        self.fc2 = nn.Linear(512, 256)\n","        self.fc3 = nn.Linear(256, 64)\n","        self.fc4 = nn.Linear(64, 8)\n","        self.fc5 = nn.Linear(8, 1)\n","\n","    def forward(self, x):\n","        x = torch.tanh(self.fc1(x))\n","        x = torch.tanh(self.fc2(x))\n","        x = torch.tanh(self.fc3(x))\n","        x = torch.tanh(self.fc4(x))\n","        x = self.fc5(x)\n","        return x"],"metadata":{"id":"r9LxfAakSCCo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define data sets and loaders"],"metadata":{"id":"qr2vfhJVVyC2"}},{"cell_type":"code","source":["train_dataset = ChessFENDataset(train_df['FEN'].values, train_df['Evaluation'].values, fen_to_feature_vector)\n","train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n","\n","val_dataset = ChessFENDataset(val_df['FEN'].values, val_df['Evaluation'].values, fen_to_feature_vector)\n","val_loader = DataLoader(val_dataset, batch_size=512)\n","\n","test_dataset = ChessFENDataset(test_df['FEN'].values, test_df['Evaluation'].values, fen_to_feature_vector)\n","test_loader = DataLoader(test_dataset, batch_size=512)"],"metadata":{"id":"hRGCbnzgTUCX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Swap to gpu if available. Please enable it in your noteobok by going to Runtime->change runtime type"],"metadata":{"id":"ez-aUSVHV1Tw"}},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"IDei1quaWB2-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Create a new model for training (No need to do this if you're just testing)"],"metadata":{"id":"NW6yawgh37_x"}},{"cell_type":"code","source":["model = NNUE().to(device)"],"metadata":{"id":"vzvq02Ti37XM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Or load our model for testing"],"metadata":{"id":"0Vb213bR4BeG"}},{"cell_type":"code","source":["our_model = NNUE()\n","our_model.load_state_dict(torch.load('/content/model.pth', map_location=device))\n","our_model.to(device)"],"metadata":{"id":"1GgnrvhX3_Es"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define parameters for training."],"metadata":{"id":"l1Kpybg_WGST"}},{"cell_type":"code","source":["\n","criterion = torch.nn.SmoothL1Loss()\n","optimizer = RMSprop(model.parameters(), lr=1e-4)\n","best_val_loss = float('inf')\n","patience = 5\n","patience_counter = 0\n","epochs = 40\n","scheduler = StepLR(optimizer, step_size=8, gamma=0.7)"],"metadata":{"id":"PIzhFGE_WIp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Train the new model**! We train with SmoothL1 loss but print MAE for visual indication (Will take a long time, skip to testing instead)"],"metadata":{"id":"5n9fMiPCWSZZ"}},{"cell_type":"code","source":["print(\"Starting\")\n","for epoch in range(epochs):\n","    model.train()\n","    train_losses = []\n","    train_mae_losses = []\n","    for x_batch, y_batch in train_loader:\n","        x_batch = x_batch.to(device)\n","        y_batch = y_batch.unsqueeze(1).float().to(device)\n","\n","        optimizer.zero_grad()\n","        preds = model(x_batch)\n","\n","        loss = criterion(preds, y_batch)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_losses.append(loss.item())\n","\n","        preds_unnorm = preds * y_train_std + y_train_mean\n","        y_unnorm = y_batch * y_train_std + y_train_mean\n","        mae_loss = torch.mean(torch.abs(preds_unnorm - y_unnorm))\n","        train_mae_losses.append(mae_loss)\n","\n","    model.eval()\n","    val_losses = []\n","    val_mae_losses = []\n","    with torch.no_grad():\n","        for x_batch, y_batch in val_loader:\n","            x_batch, y_batch = x_batch.to(device), y_batch.to(device).unsqueeze(1)\n","            preds = model(x_batch)\n","\n","\n","            loss = criterion(preds, y_batch)\n","            val_losses.append(loss.item())\n","            preds_unnorm = preds * y_train_std + y_train_mean\n","            y_unnorm = y_batch * y_val_std + y_val_mean\n","            mae_loss = torch.mean(torch.abs(preds_unnorm - y_unnorm))\n","            val_mae_losses.append(mae_loss)\n","\n","    avg_train_loss = np.mean(train_losses)\n","    avg_val_loss = np.mean(val_losses)\n","    avg_train_mae = np.mean([x.detach().cpu().numpy() for x in train_mae_losses])\n","    avg_val_mae = np.mean([x.detach().cpu().numpy() for x in val_mae_losses])\n","\n","\n","    print(f\"Epoch {epoch+1}: Train MAE = {avg_train_mae:.5f}, Val MAE = {avg_val_mae:.5f}, \"\n","          f\"Train Loss = {avg_train_loss:.5f}, Val Loss = {avg_val_loss:.5f}\")\n","    scheduler.step()\n","    if avg_val_loss < best_val_loss:\n","        best_val_loss = avg_val_loss\n","        best_model_state = model.state_dict()\n","        patience_counter = 0\n","        torch.save(best_model_state, '/content/best_model.pth')\n","        print(f\"Best model saved at epoch {epoch+1}\")\n","    else:\n","        patience_counter += 1\n","        if patience_counter >= patience:\n","            print(\"Early stopping triggered.\")\n","            break\n","\n","model.load_state_dict(best_model_state)"],"metadata":{"id":"GtHYqSOnWNGz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test our model! Printed error is MAE in centipawn. (100 points = 1 pawn value) Result is ~59."],"metadata":{"id":"73r7qY0L0fpb"}},{"cell_type":"code","source":["our_model.eval()\n","test_mae_losses = []\n","\n","\n","with torch.no_grad():\n","    for x_batch, y_batch in test_loader:\n","        x_batch = x_batch.to(device)\n","        y_batch = y_batch.to(device).unsqueeze(1).float()\n","\n","        preds = our_model(x_batch)\n","\n","        # Unnormalize predictions and targets\n","        preds_unnorm = preds * y_train_std + y_train_mean\n","        y_unnorm = y_batch * y_test_std + y_test_mean\n","\n","        mae_loss = torch.mean(torch.abs(preds_unnorm - y_unnorm))\n","        test_mae_losses.append(mae_loss)\n","\n","avg_test_mae = np.mean([x.detach().cpu().numpy() for x in test_mae_losses])\n","print(f\"Test MAE = {avg_test_mae:.5f}\")"],"metadata":{"id":"ig5g2VqA0fRA"},"execution_count":null,"outputs":[]}]}